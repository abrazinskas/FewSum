{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\">Twitter sentiment analysis with MLDP</h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we're going to cover a simple scenario on how to use the **machine learning data pipeline** (MLDP) module for **tweet polarity classification** (a.k.a. sentence-level polarity sentiment analysis). \n",
    "\n",
    "<h2> Sentiment analysis introduction </h2>\n",
    "\n",
    "\n",
    "<img src=\"img/sentiment.jpg\" alt=\"sentiment anaylsis\" width=\"50%\" style=\"float:right;margin:10px;margin-top:0px;\"/>\n",
    "\n",
    "Sentiment analysis is a very broad field, and to a great extent focuses on opinions extraction from text. For example, given product reviews, we might be interested in knowing how positive or negative they are. Alternatively, we might be interested in specifics of what users like or dislike.\n",
    "\n",
    "Polarity sentiment analysis tackles classification of textual input (tweets, reviews, documents, etc) into polarity classes, which are in most cases negative, neutral, or positive. While can appear as a relatively simple task, polarity classification models have to overcome high complexity and variation of human language. For example, sorcasm and word ambiguity are still research open problems.  \n",
    "\n",
    "Twitter as an online social platform has a vast amount of textual information useful for many business cases, e.g. trends detection, clients segmentation. On the other hand, short tweets with informal language introduce an extra complexity for machine learning (ML) models. Historically, researchers applied different rule based approaches to model text in such cases, for example by manually introducing feature functions and than using ML models to learn their significance. Nowadays, a more common approach is to learn both features and their significance automatically, for example by using a neural network. In this tutorial, we're going to be using the latter approach an use a recurrent neural model. \n",
    "\n",
    "\n",
    "<h2>Tweet polarity classification</h2>\n",
    "\n",
    "In this tutorial, we will be building an end-to-end system as shown below. \n",
    "\n",
    "<img src=\"img/system.png\" alt=\"complete system\" width=\"70%\"/>\n",
    "\n",
    "To put it simple, the system inputs a tweet and outputs a distribution over positive, negative, and neutral polarities. \n",
    "Concretely, it will consist of two main components: \n",
    "\n",
    "* **Machine learning model** - a model that classifies tweets.\n",
    "* **Data pipeline** - a pipeline cleans and prepares our data to be consumed by the model. Specifically, it reads tweets from a local storage and processes them in the online fashion to produce batches.  \n",
    "\n",
    "The ML model implemented in Keras will be provided, namely we will be using an LSTM based model for text classification. And the main focus of the tutorial will be on the second component of the system, namely the **data pipeline**, which is implemented using the **MLDP** module. \n",
    "\n",
    "<h2>Tutorial's purpose</h2>\n",
    "\n",
    "The purpose of this tutorial is to illustrate key features of the MLDP module, and show its real-case application. By the end of the tutorial we will:\n",
    "\n",
    "* Have an end-to-end system for tweet polarity classification.\n",
    "* Learn basic features of the module, such as preprocessing and processing phases. \n",
    "* Learn how to create vocabularies using the module, which are very common in NLP.\n",
    "* Familiarize yourself with some basic steps available for out-of-the-box usage.\n",
    "* Learn how to create automatically scrapped documentation of your pipelines for re-producibility of experiments. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Requirements</h2>\n",
    "\n",
    "The tutorial assumes that you've installed the ML data pipeline module, use Python 2.7, and the following modules:\n",
    "\n",
    "* Keras==2.2.3\n",
    "* nltk==3.3\n",
    "* Theano==1.0.3\n",
    "\n",
    "It can work for other versions, but without guarantees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data</h2>\n",
    "\n",
    "We're going to be using data from [SemEval-2017 Task 4](http://alt.qcri.org/semeval2017/task4/), namely the development English twitter dataset. The dataset is in the csv format, and has 3 implicit columns: ids, polarities, and tweets. An example of two tweets is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['619950566786113536',\n",
       "  'neutral',\n",
       "  \"Picturehouse's, Pink Floyd's, 'Roger Waters: The Walll - opening 29 Sept is now making waves. Watch the trailer on Rolling Stone - look...\"],\n",
       " ['619969366986235905',\n",
       "  'neutral',\n",
       "  'Order Go Set a Watchman in store or through our website before Tuesday and get it half price! #GSAW @GSAWatchmanBook']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[['619950566786113536','neutral',\"Picturehouse's, Pink Floyd's, 'Roger Waters: The Walll - opening 29 Sept is now making waves. Watch the trailer on Rolling Stone - look...\"],\t\n",
    "['619969366986235905','neutral','Order Go Set a Watchman in store or through our website before Tuesday and get it half price! #GSAW @GSAWatchmanBook']]\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains **20632 tweets**, **38k unique words**, and **3 polarity classes**. The data is located at **data/tweets.csv**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will be a recurrent neural tweet classifier as shown in the figure below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/model.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will input a processed tweet by the pipeline (a batch of tweets be be more precise) embed it using word embeddings, and run the LSTM to encode the tweet and produce a representation vector that captures semantics and syntactics. Finally, it will perform an affine transformation using a dense layer, and class scores normalization using softmax. The final output of the system is a distribution over positive, negative, and neutral classes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is already implemented using Keras, and can be directly used for tweet classification. Also, we will use a simple wrapper **interface** (*model/i_senti_lstm.py*) over the **model** (*model/senti_lstm.py*) in order to provide an additional logic for training and computation of accuracy. The train and test methods of the interface are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(self, **data_source_kwargs):\n",
    "    \"\"\"Trains the model for a single epoch.\"\"\"\n",
    "    itr = self.data_pipeline.iter(**data_source_kwargs)\n",
    "    for counter, (tweets_batch, labels_batch) in enumerate(itr, 1):\n",
    "        loss = self.model.train(tweets_batch, labels_batch)\n",
    "        if counter % 100 == 0:\n",
    "            print (\"chunk's # %d loss: %f\" % (counter, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def test(self, **data_source_kwargs):\n",
    "        \"\"\"Iterates over data batches, computes and prints accuracy [0, 1].\"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        itr = self.data_pipeline.iter(**data_source_kwargs)\n",
    "        for tweets_batch, labels_batch in itr:\n",
    "            predictions = self.model.predict(tweets_batch)\n",
    "            correct += np.sum(predictions == np.argmax(labels_batch, axis=1))\n",
    "            total += len(tweets_batch)\n",
    "        print \"accuracy: %f\" % (float(correct)/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, we need to provide the interface that feeds the model with an iterable **data_pipeline** over processed **tweet_batches** and **label_batches**:\n",
    "\n",
    "* **tweets_batch** - an integer numpy array of size *batch_size* x *sequence_length*.\n",
    "* **labels_batch** - an integer numpy array of size *batch_size* x *3*.\n",
    "\n",
    "We're going to discuss specifics of raw data processing to obtain the desired batches shortly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Pipeline Introduction</h2>\n",
    "\n",
    "A data pipeline can be understood as a sequence of steps that are applied to raw data that resides on a storage (remote or local) in order to produce formatted data batches a model requires for training and evaluation. Steps perform particular operations on data and feed subsequent steps with their output until the end of the pipeline is reached. \n",
    "\n",
    "The MLDP module allows to implement your steps in order to efficiently scale data processing on multi-core architecures, log its configuration for re-producibility, and permit re-usability of code across different machine learning projects. In our case, a more detailed setup is shown in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/architecture.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we shall scatch required operations on the raw data stored on the local storage. Afterwards, we shall construct data pipelines based on those operations for the task at hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> Preprocessing </h3>\n",
    "\n",
    "Preprocessing usually refers to the first phase operations, which are static by their nature, and can be performed only once before processing (the second phase) takes place. Some examples of preprocessing are listed below.\n",
    "\n",
    "* Get a fresh dump of data from a remote location and save it on a local storage.\n",
    "* Clean files with expensive regular expressions.\n",
    "* Perform text enrichment (POS tagging, dependency parsing, etc).\n",
    "\n",
    "In our case, the raw data file (*data/tweets.csv*) has **two undesired properties**:\n",
    "1. No column names (i.e. no header).\n",
    "2. Some tweets are wrapped in double quotes, which ideally should be removed not to confuse the model.\n",
    "\n",
    "And those two issues can be solved once for a file and later a clean version of the file can be reused for model's training and evaluation. In other words, there is no need to re-execute the preprocessing step for every model's run.\n",
    "It's especially useful when many experiments are conducted using the same model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task to be accomplished, we will be using a **preprocessing step**, which is implemented in **components/twitter_files_preprocessor.py**. The step has a simple caching, that allows to reuse a previous execution output. We're not going to dive into the details on how its implemented, but an interested reader is encouraged to check the implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Processing</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have performed a preliminary preprcessing, we can perform processing, which is a sequence of steps that alter read data-chunks in order to produce a desired output.\n",
    "\n",
    "As we discussed previously, for the task at hand, the final output should be tuples:\n",
    "\n",
    "* **tweets_batch** - an integer numpy array of size *batch_size* x *sequence_length*.\n",
    "* **labels_batch** - an integer numpy array of size *batch_size* x *3*.\n",
    "\n",
    "Specifically, our tweets should be cleaned, tokenized, tokens converted to token ids, and be padded to ensure the same *sequence_length* (a requirement to run LSTM). Labels should be also converted to ids, and then to **one-hot vectors** (a Keras requirement). One-hot vectors, are vectors of all zeros except the component corresponding to the actual class's label (it's set to 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to obtain the desired batches would be using the following processing pipeline's architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/dpp.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CSV Files Reader** - reads file(s) based on passed paths and produces raw data-chunks.\n",
    "\n",
    "**Fields Selector** - selects only *tweet* and *label* fields, as other fields are not necessary for the task.\n",
    "\n",
    "**Token Processor** - splits tweet strings into tokens, lower-cases tokens, and cleans them using regular expressions.\n",
    "\n",
    "**Vocabulary Mapper** - maps both tweet tokens and labels to integer ids.\n",
    "\n",
    "**Sequence Padder** - pads each tweets sequence with a special token id to assume the same sequence length over batch. \n",
    "\n",
    "**Formatter** - splits data-chunks into a tuple of tweet and label batches. Also, converts label ids to one-hot vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Pipelines Implementation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltoolkit.mldp.pipeline import Pipeline\n",
    "from mltoolkit.mldp.steps.readers import CsvReader\n",
    "from mltoolkit.mldp.steps.transformers import TokenProcessor,\\\n",
    "VocabMapper, FieldsSelector, Padder\n",
    "from mltoolkit.mldp.utils.util_funcs.nlp.token_cleaning import twitter_text_cleaner\n",
    "from mltoolkit.mldp.utils.util_classes import Vocabulary\n",
    "from mltoolkit.mldp.utils.util_classes.vocabulary import PAD_TOKEN\n",
    "from steps import TwitterFilesPreprocessor, FeaturesLabelsFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/tweets.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Vocabularies Pipeline</h3>\n",
    "\n",
    "First of all, we need to create vocabularies that will contain our mappings from token strings to ids and vice-versa. While one could create vocabulary classes that have an internal logic for files reading, but it becomes tricky to generalize to different file formats. In addition, the logic will have cover a possibility to process read data before creation of mappings (e.g. tokenization or tokens cleaning).\n",
    "\n",
    "Instead, we have a **decaupled** vocabulary class from data parsing relying on a data-chunk provider, which in this case will be a data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths where vocabs will be saved and later loaded from\n",
    "words_vocab_file_path = \"data/vocabs/words.txt\"\n",
    "labels_vocab_file_path = 'data/vocabs/labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating step objects\n",
    "twitter_tokenizer = TweetTokenizer()\n",
    "preprocessor = TwitterFilesPreprocessor(input_cols_number=3, tweets_indx=2,\n",
    "                                        add_header=['ids', 'labels', 'tweets'])\n",
    "csv_reader = CsvReader(sep='\\t', chunk_size=30)\n",
    "fields_selector = FieldsSelector(field_names=[\"tweets\", \"labels\"])\n",
    "token_processor = TokenProcessor(field_names=\"tweets\",\n",
    "                                 tokenization_func=twitter_tokenizer.tokenize,\n",
    "                                 token_cleaning_func=twitter_text_cleaner,\n",
    "                                 lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pipeline for vocabularies creation\n",
    "vocab_data_pipeline = Pipeline(reader=csv_reader,\n",
    "                               preprocessor=preprocessor,\n",
    "                               worker_processes_num=0, name_prefix=\"vocabs\")\n",
    "vocab_data_pipeline.add_step(fields_selector)\n",
    "vocab_data_pipeline.add_step(token_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating or loading vocabs\n",
    "words_vocab = Vocabulary(vocab_data_pipeline, name_prefix=\"words\")\n",
    "words_vocab.load_or_create(words_vocab_file_path,\n",
    "                           data_source={\"data_path\": data_path},\n",
    "                           data_field_names=\"tweets\")\n",
    "\n",
    "labels_vocab = Vocabulary(vocab_data_pipeline, add_default_special_symbols=False,\n",
    "                         name_prefix=\"labels\")\n",
    "labels_vocab.load_or_create(labels_vocab_file_path,\n",
    "                            data_source={\"data_path\": data_path},\n",
    "                            data_field_names=\"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we obtain vocabulary files for words and labels which are saved to **words_vocab_file_path** and **labels_vocab_file_path** respectively. Each file is a space separated token-count pair, and the file can be loaded next time the vocabulary is needed without re-computation.\n",
    "\n",
    "Also, **words_vocab** and **labels_vocab** are objects containg symbols corresponding to str tokens, counts and ids. In other words, each unique token has a separate symbol, which can easily accessed using those objects. We will later use them to perform mapping from tokens to ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also might be interested in the configurations/setups of our vocabularies in order log that information for experiments re-producibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#########   Words Vocabulary   #########\n",
      "\n",
      "  min_count: 1\n",
      "  max_size: None\n",
      "  sep: ' '\n",
      "  encoding: 'utf-8'\n",
      "  add_default_special_symbols: True\n",
      "  special_symbols: (keys){<PAD>, <UNK>}\n",
      "  vocab_size: 38388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(words_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########   Labels Vocabulary   #########\n",
      "\n",
      "  min_count: 1\n",
      "  max_size: None\n",
      "  sep: ' '\n",
      "  encoding: 'utf-8'\n",
      "  add_default_special_symbols: False\n",
      "  special_symbols: {}\n",
      "  vocab_size: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(labels_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also save/log information about the pipeline that was used for our vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#################################################################\n",
      "#                    VOCABS PIPELINE'S SETUP                    #\n",
      "#################################################################\n",
      "\n",
      "  worker_processes_number: 0\n",
      "\n",
      "#################   TwitterFilesPreprocessor   ##################\n",
      "\n",
      "  input_cols_number: 3\n",
      "  output_folder: 'data/clean_tweets/'\n",
      "  input_sep: '\\t'\n",
      "  output_sep: '\\t'\n",
      "  add_header: ['ids', 'labels', 'tweets']\n",
      "  tweets_indx: 2\n",
      "  encoding: 'utf-8'\n",
      "\n",
      "######################   FieldsSelector   #######################\n",
      "\n",
      "  field_names: ['tweets', 'labels']\n",
      "\n",
      "######################   TokenProcessor   #######################\n",
      "\n",
      "  field_names: ['tweets']\n",
      "  tokenization_func: nltk.tokenize.casual.TweetTokenizer.tokenize\n",
      "  token_cleaning_func: twitter_text_cleaner\n",
      "  token_matching_func: None\n",
      "  lower_case: True\n",
      "\n",
      "#################################################################\n",
      "#                                                               #\n",
      "#################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(vocab_data_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The print-out to a large extend is **scrapped automatically** from objects. However, each step could be enriched to produce a better signature by overriding a specific method (**get_signature_attrs**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training and Evaluation Pipeline</h3>\n",
    "\n",
    "Once we have vocabulary objects, we can proceed creating the pipeline necessary for model's training and evaluation. Also, we will execute data parsing on a separate process by setting **worker_processes_num=1** in the pipeline. This configuration is especially useful if a model is trained on a GPU, so a CPU can be used in parallel for data-preparation by filling a buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra steps for training and evaluation\n",
    "mapper = VocabMapper(field_names_to_vocabs={\"tweets\": words_vocab,\n",
    "                                            \"labels\": labels_vocab})\n",
    "padder = Padder(field_names=\"tweets\", pad_symbol=words_vocab[PAD_TOKEN].id)\n",
    "formatter = FeaturesLabelsFormatter(features_field_name=\"tweets\",\n",
    "                                    labels_field_name=\"labels\",\n",
    "                                    classes_number=len(labels_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the actual pipeline\n",
    "dev_data_pipeline = Pipeline(reader=csv_reader, preprocessor=preprocessor,\n",
    "                             worker_processes_num=1, name_prefix=\"dev\")\n",
    "dev_data_pipeline.add_step(fields_selector)\n",
    "dev_data_pipeline.add_step(token_processor)\n",
    "dev_data_pipeline.add_step(mapper)\n",
    "dev_data_pipeline.add_step(padder)\n",
    "dev_data_pipeline.add_step(formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##############################################################\n",
      "#                    DEV PIPELINE'S SETUP                    #\n",
      "##############################################################\n",
      "\n",
      "  worker_processes_number: 1\n",
      "  output_buffer_size: 5\n",
      "\n",
      "################   TwitterFilesPreprocessor   ################\n",
      "\n",
      "  input_cols_number: 3\n",
      "  output_folder: 'data/clean_tweets/'\n",
      "  input_sep: '\\t'\n",
      "  output_sep: '\\t'\n",
      "  add_header: ['ids', 'labels', 'tweets']\n",
      "  tweets_indx: 2\n",
      "  encoding: 'utf-8'\n",
      "\n",
      "#####################   FieldsSelector   #####################\n",
      "\n",
      "  field_names: ['tweets', 'labels']\n",
      "\n",
      "#####################   TokenProcessor   #####################\n",
      "\n",
      "  field_names: ['tweets']\n",
      "  tokenization_func: nltk.tokenize.casual.TweetTokenizer.tokenize\n",
      "  token_cleaning_func: twitter_text_cleaner\n",
      "  token_matching_func: None\n",
      "  lower_case: True\n",
      "\n",
      "######################   VocabMapper   #######################\n",
      "\n",
      "  symbols_attr: 'id'\n",
      "  field_names_to_vocabs: {tweets: Vocabulary({min_count: 1, max_size: None, sep: ' ', encoding: 'utf-8', add_default_special_symbols: True, special_symbols: (keys){<PAD>, <UNK>}, vocab_size: 38388}), labels: Vocabulary({min_count: 1, max_size: None, sep: ' ', encoding: 'utf-8', add_default_special_symbols: False, special_symbols: {}, vocab_size: 3})}\n",
      "\n",
      "#########################   Padder   #########################\n",
      "\n",
      "  field_names: ['tweets']\n",
      "  pad_symbol: 38386\n",
      "  symbol_to_mask: None\n",
      "  padding_mode: 'both'\n",
      "  axis: 1\n",
      "  new_mask_fn_suffix: 'mask'\n",
      "\n",
      "################   FeaturesLabelsFormatter   #################\n",
      "\n",
      "  feature_field_name: 'tweets'\n",
      "  labels_field_name: 'labels'\n",
      "  classes_number: 3\n",
      "\n",
      "##############################################################\n",
      "#                                                            #\n",
      "##############################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dev_data_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Combining all together </h2>\n",
    "\n",
    "Now when we have all components, we're going to combine them to the end-to-end system that is trained and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from model import ISentiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing before training\n",
      "accuracy: 0.496897\n",
      "training the model\n",
      "epoch 1\n",
      "chunk's # 100 loss: 0.985782\n",
      "chunk's # 200 loss: 0.956783\n",
      "chunk's # 300 loss: 0.878575\n",
      "chunk's # 400 loss: 1.036046\n",
      "chunk's # 500 loss: 0.834549\n",
      "chunk's # 600 loss: 0.778580\n",
      "accuracy: 0.732858\n",
      "epoch 2\n",
      "chunk's # 100 loss: 0.972646\n",
      "chunk's # 200 loss: 0.655704\n",
      "chunk's # 300 loss: 0.595417\n",
      "chunk's # 400 loss: 0.747819\n",
      "chunk's # 500 loss: 0.640200\n",
      "chunk's # 600 loss: 0.473408\n",
      "accuracy: 0.817137\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "i_model = ISentiLSTM(dev_data_pipeline)\n",
    "i_model.init_model(words_vocab_size=len(words_vocab), input_dim=50,\n",
    "                   lstm_hidden_dim=120,\n",
    "                   number_of_classes=len(labels_vocab),\n",
    "                   mask_symbol=words_vocab[PAD_TOKEN].id)\n",
    "print(\"testing before training\")\n",
    "i_model.test(data_path=data_path)\n",
    "print(\"training the model\")\n",
    "for epoch in range(1, epochs+1):\n",
    "    print \"epoch %d\" % epoch\n",
    "    i_model.train(data_path=data_path)\n",
    "    i_model.test(data_path=data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model should train for two epochs, and we should observe an improvement in terms of accuracy and loss. That indicates that all parts were combined correctly. Congrats :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusion</h2>\n",
    "\n",
    "In this tutorial we have covered how to create a data pipeline for a LSTM based sentiment analysis model. We covered some of its basic features, and processing steps that can be useful for data parsing. Hope you have enjoyed the tutorial, and will find the module useful for your own ML projects. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
